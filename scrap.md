Traditional machine learning models take in data and output predictions. In contrast, many newer models incorporate mechanisms that take in _**state**_ and output _**state**_. This is actually the defining characteristic of a deep network: inner layers of a network operate on state, not directly on data or predictions. I think that a model's ability to manipulate, mutate, and store state is one of the main determiners of intelligence. Discussion of a specific example will hopefully strengthen this claim:

When I ride a bicycle, my brain seems to upload what we might call a bicycle-riding "program" from memory. This "uploading" mechanism seems to take cues not from external sensory inputs but from some kind of internal signal. Additionally, my brain draws on complex memories, such as the meaning of traffic signs, that are relevant to the activity. It also attends carefully to certain sensory signals like the movements of cars, while ignoring other signals that might be important in other circumstances, like expressions on faces. Given this complex system of conditioning mechanisms, my behavior on the bicycle from moment to moment requires little attention or effort.

Generally, in the moment of activity while performing a certain task, behavior is often (always?) automatic. Indeed, the progress of machine learning research suggests that current architectures are capable of mastering many moment-to-moment inference tasks. What distinguishes human intelligence is _not_ the specific architecture for performing inference, but the _supporting_ architectures that set the stage for inference. In computational terms, the significant element is not the CPU, but the programs that populate the computer memory and establish the state of the system before the computation is even performed.

These observations drew me to research on "learning to learn" or "meta-learning." This topic encompasses all mechanism that improve the _learning process_ irrespective of the specific task being learned. The power of this meta capability becomes evident in applications that require rapid learning, for example, one-shot learning. In these applications, a model does not have time to slowly learn the inference function using gradient descent. Instead, models have to learn a more abstract mapping: from inputs to hidden states that then facilitate the rapid learning of the actual function. For example, when a person learns a new word, the brain's rapid assimilation of the word into vocabulary clearly suggests the presence of more a powerful _learned_ program specifically responsible for vocabulary acquisition.

One of my favorite treatments on this topic is Dr. Tenenbaum's recent paper, "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation." The paper describes an algorithm that learns to break a task into subtasks using goals. One model, the "meta-controller" learns to choose goals based on the current state in order to maximize extrinsic rewards from the environment. Meanwhile a second model, the "controller," learns a policy (which Tenenbaum calls an "option") for accomplishing the goal specified by the meta-controller using conventional reinforcement learning. The paper demonstrates the performance of the algorithm in two domains: a simple state machine with probabilistic transitions and the classic Atari game, Montezuma's Revenge (in the latter, the algorithm uses convolutional neural networks to approximate the Q-function).

What excites me about this paper is the paradigm that it presents for manipulating and combining programs. Tenenbaum's _options_ are very similar to _programs_ as I described them in earlier paragraphs. Tenenbaum's experiments concretely demonstrate the power of this approach to meta-learning. However, the paper concedes that it has some limitations. Specifically, in Montezuma's Revenge, goals are specified as "objects" in the environment (e.g. key, door, etc.), picked out by a custom object detector. The paper suggests that the approach would generalize better if methods existed for unsupervised object detection, but one can even imagine situations in which visual objects were not a good specification for goals at all (for example a natural language task).

 <!-- This paper offers one of the most compelling solutions to reinforcement learning problems with sparse feedback. The division of labor between controller and meta-controller simplifies the task for both: the controller doesn't have to learn how to choose goals and the meta-controller doesn't have to learn how to perform them. -->

A naive solution is for the meta-controller to learn a dynamic _embedding_ of goals. In the paper, the meta-controller selects among pre-selected goals (e.g. visual objects in Montezuma's Revenge). Instead, the meta-controller might output an encoded representation of a goal (a real-valued vector without any obvious meaning) that the controller would then learn to interpret, conditioning its choice of action on these goal representations. Finally, the controller would need some kind of feedback when it accomplishes a goal. To this end, a third 'critic' network might produce intrinsic rewards for the controller, conditioning these rewards on the meta-controller's choice of goal and the current state of the environment.

Unfortunately, this naive solution is unlikely to work: how can the critic network learn to reward the actions of the controller until the actual task is accomplished and some feedback is received from the environment? Until then, exploration will be random and the model will suffer the same difficulties as an ordinary deep Q network---never reaching any objectives, never receiving any feedback, and therefore never learning.

<!-- As one considers less structured approaches to the specification of goals, it becomes evident that hard-coding an association between goals and visual objects tells the model a great deal about its environment, things that it would probably never learn otherwise: that rewards are associated with specific locations in the environment that tend to be diffuse. -->

Unstructured approaches to goal specification all seem to lead to a chicken-and-the-egg problem: complex tasks must be broken down into subgoals to be accomplished; however, it is seemingly impossible to learn this breakdown without accomplishing the task at least once and getting some feedback.

This is the problem that I would like to study: how does a model learn to dynamically plan a series of goals without first accomplishing the global task? I believe that this process is recursive and hierarchical: in the base case, the model learns simple tasks through random exploration and ordinary reinforcement learning; then more complex tasks are mastered by composing previously-learned, simpler tasks. I would like to explore this idea using Tenenbaum's controller/meta-controller architecture.

<!-- The question remains: how do we "compose" simpler tasks? Again, Tenenbaum's work provides many exciting avenues for inquiry. The "Intrinsic Motivation" paper already demonstrated the composition of goals through sequencing. But what if goals were encoded vectors? How Perhaps if goals were represented as abstract embeddings, emergent behavior could be produced in the system by combining Not only can his architecture combine goals through sequencing, put potentially the model could learn to interpolate previously learned options (goal-oriented policies) when  -->


 <!-- The defining characteristic of a program is the _goal_ that it learns to achieve---if two different programs achieve the same goal, they are still essentially interchangeable in the framework that I described in the previous paragraph. Therefore, if we learn to manipulate goals, we learn to manipulate programs. -->

<!-- It is worth noting that the kind of state mutation that I am interested in is distinct from the mutation that occurs through normal training. Learning by gradient descent is slow and stops once training is complete. In contrast, a more advanced and expressive network like an LSTM can perform mutations on its hidden state even after training is complete, without receiving feedback. -->


<!-- My research interests are based on this premise: the mental architecture that supports generalized, human-like intelligence is not very different in kind from a recurrent neural network. The distinguishing characteristic of the human mind is its ability to dramatically alter its _state_ to suit whatever task the circumstances call for.

In the moment of inference, the architecture of the brain performs a feedforward pass through a neural network. What distinguishes the brain from any other neural model is not its activity at this moment, but the _context_ in which it performs the action. In other words, unlike a standard neural network, the ... -->
