If someone claimed that modern machines were intelligent, how would we disprove them? We could point to tasks that humans can perform that machines cannot -- but this is a two-way street: there are tasks that machines perform that humans cannot. And specific examples can never get to the heart of the matter. We could use terms like "generalized" intelligence, but what exactly does that mean? Maybe something to do with emergent behavior, activity in the absence of structure, the ability to identify intermediate objectives in the pursuit of distant ones.

To be clear, my interest is not merely philosophical. I am interested in _mechanisms_ that make this kind of cognition possible. I believe that developments in machine learning bring us closer to understanding these questions. In particular, there is a development common to many recent innovations that I am very excited about. In contrast to traditional machine learning algorithms, which take in data and output predictions, many of the most successful models in recent research incorporate mechanisms that take in _state_ and output _state_. This is actually the defining characteristic of a deep network: inner layers of a network operate on state, not directly on data or predictions.

As time goes on, the mechanisms by which deep networks manipulate their own state seem to become increasingly advanced. I do not think this is simply a byproduct of models becoming generally more complex. I speculate that a model's ability to manipulate, mutate, and store state is one of the main determiners of intelligence. Here is an example that I hope will strengthen this claim:

When I ride a bicycle, my brain seems to upload what we might call a bicycle-riding "program" from memory. This "uploading" mechanism seems to take cues not from external sensory inputs but from some kind of internal signal (maybe something we could call _volition_?). Additionally, my brain draws on complex memories, such as the meaning of traffic signs, that are relevant to the activity. It also attends carefully to certain sensory signals like the movements of cars, while ignoring other signals that might be important in other circumstances, like expressions on faces. Given this complex system of conditioning mechanisms, my behavior on the bicycle from moment to moment requires little attention or effort.

This seems to be true in general: in the moment of activity while performing a certain task, behavior is often (always?) automatic. Indeed, the progress of recent research suggests that current architectures for learning are capable of mastering many moment-to-moment inference tasks. What distinguishes human intelligence is _not_ the specific architecture for performing inference, but the _supporting_ architectures that set the stage for inference. In computational terms, the significant element is not the CPU, but the programs that populate the computer memory and establish the state of the system before the computation is even performed.

It is worth noting that the kind of state mutation that I am interested in is distinct from the mutation that occurs through normal training. Learning by gradient descent is very slow and stops once training is complete. In contrast, a more advanced and expressive network like an LSTM performs mutations on its hidden state while processing inputs even after training is complete, without receiving feedback.

The power of this meta capability becomes evident in applications that require rapid learning, especially zero- or one-shot learning. In these applications, a model does not have time to slowly learn the inference function using gradient descent. Instead, models have to learn a more abstract mapping: from inputs to hidden states that then facilitate the rapid learning of the actual function. For example, when a person learns a new word, the brain's rapid assimilation of the word into vocabulary clearly suggests the presence of more a powerful _learned_ program specifically responsible for vocabulary acquisition.

What happens when this idea is taken to its logical extreme? LSTMs operate on their own parameters, but an even more powerful kind of learning is learning that operates on programs themselves. Humans seem to exhibit this capability: a person familiar with biking and with skateboarding will immediately figure out how to ride a scooter. That person is not only accessing abstract knowledge but somehow accessing and combining existing programs that may even be beyond the mind's logical comprehension. Though slow, gradient-descent-like learning might gradually improve the abilities of the scooter rider, the initial encounter with the scooter will does not benefit from this kind of learning. Indeed, for most specific tasks, learning derived from past experience -- not learning specific to the current task -- probably accounts for the _bulk_ of overall learning.


<!-- My research interests are based on this premise: the mental architecture that supports generalized, human-like intelligence is not very different in kind from a recurrent neural network. The distinguishing characteristic of the human mind is its ability to dramatically alter its _state_ to suit whatever task the circumstances call for.

In the moment of inference, the architecture of the brain performs a feedforward pass through a neural network. What distinguishes the brain from any other neural model is not its activity at this moment, but the _context_ in which it performs the action. In other words, unlike a standard neural network, the ... -->
