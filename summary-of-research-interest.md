I would like to understand how the mind creates new ideas from old ones. Of course, some first ideas must come from scratch. In fact, I think that recent work in machine learning has explained a lot about how this might happen []. But after the mind has a few basic ideas to work with, it seems that most learning comes from combining old ideas, not creating altogether new ones [].

This problem is interesting to me because it relates to an idea that used to be the centerpiece of AI research and now scarcely appears in the literature: reason. If we draw comparisons with the mind, the kind of direct learning from the environment or from inputs that most machine learning algorithms perform is similar to _habit_ or _conditioning_. In contrast, _reason_ somehow organizes these simple elements of learning into extensive structures of knowledge that generalize powerfully [].

There are two kinds of combination that I am especially interested in: combination through interpolation and combination through sequencing. In the former, two ideas are somehow intermixed to form a new one. A good example is syllogism []. In the latter, tasks are sequenced through planning to perform a more complex task [].

For my doctoral research, I am specifically interested in the problem of dynamically reducing a larger tasks into subtasks. This is a very difficult problem because the space of possible subtask decompositions is very large. If we do not constrain it somehow, we are no better off than random Îµ-greedy exploration. At the same time, I do not want to use any domain-specific constraints and would prefer reinforcement learning over supervised learning.

To begin the inquiry, I propose an algorithm that includes a controller and a meta-controller []. Initially the controller would learn a repertoire of simple tasks through ordinary (Deep) Q-Learning. Meanwhile, the meta-controller would search through the space of previously learned tasks to learn new, more complex tasks. The repertoire would grow with each new task that the algorithm learned, simple or complex. In order for the model to operate in environments with sparse feedback, it would receive some kind of reward for learning new skills in the absence of rewards from the environment.
