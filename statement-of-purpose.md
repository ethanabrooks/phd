<!-- OXFORD: You may submit either a research proposal comprising a detailed outline of your intended research or, if you are not yet ready to submit a full proposal, you should submit a statement of research interests, in English, describing the general area of research in which you are interested. You should summarize the research in this area that you are aware of, referring to existing papers where appropriate. You could also describe a research problem and your initial ideas on research work towards solving this or open problems. -->
<!-- MIT: what you would like to study -->
Research Objectives

I am interested in the context in which tasks are performed. When I drive a car, how does my mind access previously learned driving skills and what internal signal cues this process? How do I retrieve memories relevant to the activity, like the meaning of traffic signs or the dynamics of moving vehicles? How does the mind orchestrate component tasks like steering, navigation, and observation? These are the questions that distinguish an autonomous driving algorithm from a thinking human behind the wheel.

Despite the seemingly unstoppable march of its conquests, machine learning is mostly confined to domains in which task context is static. Here, context refers to the information that transcends specific tasks, but organizes and governs the programs for executing them. Mechanisms for establishing and mutating context include memory, planning, and abstract thought. Tasks that depend heavily on context tend to be more difficult, including one-shot learning, multi-task learning, planning, variable binding, etc.

<!-- Drawing an analogy between the mind and a computer, most machine learning focuses on the CPU, while the programs that populate computer memory and establish the state of the system before the computation is even performed remain a mystery. I would like to understand those programs: how they are stored -->

I would like to design machines that emulate the mind's ability to create and modify context: how it cues task-specific programs, how it accesses relevant knowledge from the morass of memory accumulated over a lifetime, and how it orchestrates these processes to accomplish large, complex tasks. The specific problem that I am interested in for my doctoral research at MIT is how to combine constituent tasks to perform larger complex ones.

Several recent works have deeply influenced my thinking on these subjects. In "Neural Program Interpreters," De Freitas et al. train a model to store task-specific programs in memory, and to dynamically switch between these programs in order to perform complex tasks like addition of large numbers. This paper concretely demonstrates the capacity of a model to encode entire programs in compact hidden state vectors. It also demonstrates that a deep recurrent neural network can learn to choose among these programs in the performance of larger tasks, much like the human driver who integrates subtasks like pedaling and steering.

In “Control of Memory, Active Perception, and Action in Minecraft,” Singh et al. train a model to perform complex, sequence-based tasks in 3D maze environments (designed in Minecraft) by combining deep Q-learning with an external memory architecture similar to the one that Sukhbaatar et al. present in "End-To-End Memory Networks." With the aid of this memory architecture, the model is able to perform tasks with long temporal dependencies that thwart ordinary deep Q-networks. Moreover, Minecraft provides a rich and varied environment full of possibilities for deep RL research.

Finally, in "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation," Tenenbaum et al. train a model, _h-DQN_, to master a relatively complex video game (Montezuma's Revenge) by learning to decompose the game into simpler subgoals. This paper demonstrates that a model can accomplish more complex tasks with a division of labor between parts responsible for choosing goals and parts responsible for executing the associated tasks. One benefit of this approach is that it diminishes the sparsity of feedback. A second benefit is that it allows the different parts of the model to specialize in relatively simple tasks but achieves complexity through their combination.

I am interested in extending these works to perform tasks

- that are too complex to be discovered by purely random exploration.
- that do not depend on domain-specific constraints on the goal/subtask space.
- that are ideally learned with reinforcement learning, not supervised learning.

The goal of this research is a learner capable of autonomously navigating a complex environment, learning new skills without a human specifying them and with minimal supervision or feedback. I consider this an exciting and attainable next step in artificial intelligence research.

Background

The following traces the trajectory that led me to deep learning and the questions that I am proposing for research. I will begin with college, where the ideas that later emerged as professional interests began to take shape.

I attended St. John's College, a small, dedicated liberal arts school, where I studied primary texts in mathematics, science, philosophy, and literature, spanning ancient times to the twentieth century. All classes were discussions, and it was not uncommon for a particularly alluring question to follow students outside the classroom. During this time, I developed interest in epistemology, especially the notion of a priori knowledge. I developed strong sympathies with empiricists like David Hume and George Berkeley who claimed that all knowledge came from sense perception. However, the possibility that this form of knowledge depended on certain a priori prerequisites, as Immanuel Kant claimed, always enticed me.

A second passion that I discovered at St. John's was mathematics. Freshman year, reading Euclid rekindled an earlier fascination with proof that grew steadily through college. One of my most exciting memories is studying non-Euclidean geometry and confronting the reality that, without contradiction, straight lines could converge without meeting. That such a seeming paradox could be true was mysterious and wonderful. I wrote my senior thesis on the first of Richard Dedekind’s "Essays on the Theory of Numbers", and audited Calculus II at the Naval Academy.

After college, as a Marine Officer, I learned to lead people, manage projects, and achieve results with minimal supervision. In the Marine Corps, I combined a new practical set of skills with the skills of intellect that I gained at St. John’s. I also faced experiences that were completely foreign. Before the Marine Corps, I had played few team sports and spent little time in the outdoors. In my first few months in training, I led patrols in hostile villages, held a Jirga with Afghan-roleplayers and a translator, and conducted live-fire exercises in urban settings. I graduated every training school in the top fifth of my class and, on performance evaluations, consistently ranked among the best of my peers.

Once I reached my duty station, I hoped to continue my studies in mathematics. Because long work hours prevented me from taking classes at the local university, I began to take free courses on MIT OpenCourseWare. I made time for math before and after work and took my workbook on exercises to study during downtime. I completed courses in multivariable calculus, differential equations, and linear algebra, including all readings, assignments, and exams. I discovered computer science through the MIT course, “Artificial Intelligence,” which revealed a new approach to the questions of epistemology that had lingered since college. At last, there seemed to be a way to ask these questions with concrete language and to assess the answers with measurable results. I found Python and Java classes online, and began to teach myself programming.

During my time in the military, I taught myself a little about neural networks, but the entire field of machine learning still seemed like a dark art. Subsequently, at the University of Pennsylvania, my interest in machine learning and especially deep learning clarified itself very quickly. During the Fall semester, I took a course in Machine Learning and certain ideas took shape in my mind that I will spend my life exploring.

I was interested in the foundations of cognition, not necessarily the functionality of the brain, as studied by neuroscience. On the other hand, fields like psychology or philosophy had too little traction on the details of process and mechanism. Machine learning addressed an important issue that is at the heart of cognition, namely pattern recognition---a fundamental mechanism for compressing the overwhelming volume of sense data. However, deep learning in particular seemed to provide building-blocks for modeling the full range of cognitive mechanisms. In the context of traditional machine learning, neural networks hardly distinguished themselves among a host of pattern-matching algorithms. Deep learning repurposed them as building-blocks that could be combined into structures of arbitrary complexity and functionality. In these universal function approximators, deep learning had perhaps discovered the fundamental unit of learning, the DNA of cognition itself.

My research with Dr. Lyle Ungar was one of my most formative periods at Penn. It was my first exposure to research and to deep learning source code. In conversations with Dr. Ungar and my research partners, the logic of deep network architectures began to cohere in my mind. I began to understand how a network can compress incoming information in a bottleneck layer, enabling it to generalize better; that a network can repurpose the information inside a hidden state vector by passing this vector through different transformations, using the same information to make a prediction, set the value of a gate, or approximate a hashing function. The choices made by researchers in papers that I read emerged from obscurity and I began to imagine network designs of my own. My only regret from this period was that I was unable to focus exclusively on my research due to the competing demands of schoolwork.

I also discussed my future with Dr. Ungar. It became clear that I would only have the opportunity to pursue my interests without distraction in a doctoral program. Moreover, this would generate opportunities after graduating for further research either in academe or industry. The following summer, I interned with the Siri team at Apple--my first internship in the tech industry. I had the opportunity to work full-time on deep learning, and confirmed what I suspected, that I loved the work and had a natural aptitude for it. I was able to contribute several new memory-based deep architectures to the code-base and was given the opportunity to present my work to the VP of the Siri team.

<!-- I have a track-record of successfully taking on new endeavors. I taught myself mathematics and programming while serving successfully as an officer in the Marine Corps. I established a successful academic track-record at Penn, fresh out of the military. I made significant contributions as an intern at Apple--my first internship in the tech industry. One of my proudest accomplishments is a recent project that .  -->

One of my proudest accomplishments is a recent project in Rust, a difficult but exciting, new programming language that I had never used before. In order to better understand software like Tensorflow and Torch that most deep learning code depends on, I decided to implement my own computational graph program, including backpropogation and integration with CUDA and CUBLAS, from the ground up. The project can be viewed at https://github.com/lobachevzky/lstm. This project is evidence of my enduring fascination with deep learning, my general programming ability, and my ability to fundamentally understand the algorithms and hardware that deep learning depends on. I also believe that this project is indicative of less tangible qualities of intellectual courage: a willingness to take risks and dive into unfamiliar projects.

I am applying to MIT primarily because Dr. Tenenbaum is doing some of the most interesting research at the intersection of machine learning and cognitive science. Also, from my first introduction to MIT through OpenCourseWare, I noticed that professors approached subjects philosophically, interested in not only techniques and methodologies but also the larger, governing ideas. I have no doubt that MIT will live up to its reputation for scientific rigor, but I hope that it will also demonstrate a commitment to inquiry.

PhD research at MIT will be the culmination of the experiences I have described. At St. John’s, I planted intellectual roots and seeds of curiosity. In the Marine Corps, I learned to put intellect into action. At Penn, I developed a strong foundation in computer science. Throughout, I have demonstrated commitment to my passions, never compromising my pursuit of intellectual fulfillment. At MIT, researching a subject that has completely engrossed me, among other ambitious scholars, I anticipate a new level of fulfillment.
