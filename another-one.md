If someone claimed that modern machines were intelligent, how would we disprove them? There are many human tasks beyond the capacity of computers, but there are also many tasks in which computers vastly outperform humans. My hypothesis is that mastery of specific tasks is not what distinguishes human intelligence but rather our intricate ability to mutate the _context_ in which we learn and perform those tasks.

By context, I mean the mental state that specific inference depends on. For example, when I ride a bicycle, my brain uploads some kind of bicycle-riding "program" from memory. This "uploading" mechanism takes cues primarily from some kind of internal signal, not from external sensory inputs. Additionally, my brain draws on complex memories, such as the meaning of traffic signs, that are relevant to the activity. It attends carefully to certain sensory signals like the movements of cars, while ignoring other signals that might be important in other circumstances, like expressions on faces. Given this complex system of conditioning mechanisms, my behavior on the bicycle from moment to moment is almost automatic.

The progress of machine learning research suggests that current architectures are capable of mastering many moment-to-moment inference tasks. The tasks that humans consistently outperform machines in are those that require complex _supporting_ systems to set the stage for inference. In computational terms, the critical element is not the CPU, but the programs that populate the computer memory and establish the state of the system before the computation is even performed.

I am interested in understanding how the mind cues task-specific programs, how it accesses relevant knowledge from the morass of memory accumulated over a lifetime, and how it orchestrates these processes to accomplish large, complex tasks. The specific problem that I am interested in for my doctoral research is how to combine simple tasks to perform larger complex ones.

Several recent works have deeply influenced my thinking on these subjects. In "Neural Program Interpreters," De Freitas et al. train a model to store task-specific programs in memory, and to dynamically switch between these programs in order to perform complex tasks like addition of large numbers. In "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation," Tenenbaum et al. presented a model, _h-DQN_, that masters a relatively complex video game (Montezuma's Revenge) by learning to decompose the game into simpler subgoals. In “Control of Memory, Active Perception, and Action in Minecraft,” Singh et al. train a model to perform complex, sequence-based tasks in 3D maze environments by combining deep Q-learning with an external memory architecture similar to the one that Sukhbaatar et al. presented in "End-To-End Memory Networks."

I am interested in extending these works to perform tasks
- that are too complex to be discovered by purely random exploration.
- without any domain-specific constraints on the goal/subtask space.
- ideally with reinforcement learning, not supervised learning.

I propose a model that includes a controller and a meta-controller, like Tenenbaum's h-DQN, with the meta-controller responsible for specifying goals and the controller responsible for executing them.

I would also like to borrow several ideas from an earlier work, "Intrinsically Motivated Reinforcement Learning." In this work, Singh et al. train a model to learn complex tasks in a toy environment with discrete states. The model receives rewards for discovering "salient" states and these rewards are proportional to the unexpectedness of the state.

Inspired by this approach, I would like to use artificial curiosity as a heuristic to encourage broader exploration. In Singh's paper, salient states are hard-coded. I would like to explore more dynamic criteria. For example we might train an additional network to predict state transitions and use this to assess unexpectedness. That is, if the network predicted a low probability for a transition, this would indicate that something unexpected and therefore salient had occurred.

Another important feature of the model proposed in this paper is that it learns higher-order actions with increasing levels of abstraction. For example, to turn on a light, the model must perform a three-step sequence of actions. However, after discovering this salient event, the "turn on light" action becomes atomic, and the model can cue the associated subroutine as a single action. I would like to develop a model that similarly learns to perform abstraction over complex tasks. The three works that I originally mentioned all demonstrate methods for encoding and therefore compressing complex actions which could be applied to this purpose.

I am interested in exploring one more direction of research that is entirely new, as far as I know: deeper hierarchy in the system of learners. Like a deep network with many layers, the system might benefit from multiple layers of controllers, with higher level controllers training lower level controllers and steadily pushing increased responsibility to them over time, with more complex goals and sparser feedback.

<!-- In this case, the question is not how did I learn to ride a bike. The question is how my mind provided the context in which the task was performed. One piece of evidence that this is a critical factor for intelligence is that as deep learning algorithms take on more advanced tasks, they also develop more complex mechanisms for mutating their state. For example, a recurrent neural network's internal state is primarily a function of the input that it receives on a given time-step. In contrast, the specific inputs to an LSTM on a given time step have much less influence on the cell's output relative to other factors like the hidden state or the contents of memory. -->

<!-- There are many ways in which we could study this problem of providing context for inference, but my favorite question is the problem of decomposing a task into subtasks. -->
